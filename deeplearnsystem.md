Deep Learning learning series

Learn from Li Mu

https://zh-v2.d2l.ai/chapter_preliminaries/index.html

![alt text](image.png)

Course Planning with video link:

https://courses.d2l.ai/zh-v2/


10. 多层感知机
普通感知机：给定输入x，权重w和偏移b，那么感知机就对应输出1或者0（二分）
训练感知机：等价于使用批量大小为1的梯度下降
不能拟合XOR函数，导致第一次AI寒冬

多层感知机
使用隐藏层和激活函数得到非线形模型
常用激活函数是sigmoid，ReLU
使用Softmax处理多类分类
模型一般是先扩张再压缩，更不容易失真；

11. 模型选择 + 过拟合及欠拟合

验证数据集validation dataset - 用来评估模型好坏的数据集（重要：不要跟原模型训练数据混在一起
测试数据集test data - 只用一次的数据集，最后的试装
K-则交叉验证 - 没有足够的数据的时候，把训练数据分割成K块（一般选择5或10）
小结：
训练数据集：训练模型参数
验证数据集：选择模型超参数
非大数据集上通常使用k-折交叉验证

数据复杂且模型容量小->欠拟合
数据简单且模型容量大->过拟合
![](image-1.png)

数据复杂度：样本个数，每个样本的元素个数，时间和空间结构，多样性
**总结：**
-欠拟合是指模型无法继续减少训练误差。过拟合是指训练误差远小于验证误差。
-由于不能基于训练误差来估计泛化误差，因此简单地最小化训练误差并不一定意味着泛化误差的减小。机器学习模型需要注意防止过拟合，即防止泛化误差过大。
-验证集可以用于模型选择，但不能过于随意地使用它。

-我们应该选择一个复杂度适当的模型，避免使用数量不足的训练样本。
-一般70%作为训练数据集（5-则交叉验证），30%验证数据集








BERT 学习

BERT是芝麻街动画里的主人翁，19年出的这个模型

NLP里的迁移学习：使用预训练好的模型来抽取词/句子的特征

BERT就是一个只有编码器的Transformer，针对NLP的微调设计
- Base: #blocks=12,hidden size = 768,#heads=12,#parameter=110M
- Large: #blocks=24,hidden size = 1024,#heads=16,#parameter=340M
 
 预训练任务1：带掩码的语言模型
 预训练任务2: 下一个句子预测（50%正确的对子）
小结：
BERT针对NLP微调设计
基于Transformer的编码器做了修改：
- 模型更大，训练数据更多
- 输入句子对，片段嵌入，可学习的位置编码
- 训练时使用两个预训练任务
  - 带掩码的语言模型
  - 下一个句子预测

代码实现端：










